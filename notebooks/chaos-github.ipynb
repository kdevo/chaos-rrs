{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAOS for GitHub\n",
    "\n",
    "\n",
    "The following block is just a common boilerplate to ensure the modules are loaded properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if  os.path.basename(root) == 'chaos':\n",
    "    if root not in sys.path:\n",
    "        print(\"Add project root to syspath\")\n",
    "        sys.path.append(root)\n",
    "    print(\"Change working directory to project gitchaos example\")\n",
    "    os.chdir(os.path.join(root, 'examples/gitchaos'))\n",
    "\n",
    "    \n",
    "import chaos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging.config\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "\n",
    "from chaos.process.clean.df import DFCleaner\n",
    "from chaos.process.clean.humanize import TextConverter, ColumnFormatType\n",
    "from chaos.process.extract.graph import GraphEdgeMapper, GraphPopularityExtractor\n",
    "from chaos.process.extract.name import NameToGenderExtractor\n",
    "from chaos.process.extract.nlp import NLPEntityExtractor, NLPTokenExtractor\n",
    "from chaos.process.extract.reduce import MostUsedExtractor\n",
    "from chaos.process.pipeline import SequentialDataPipeline\n",
    "from chaos.recommend.candidates import InteractionCG, DMCandidateRepo, StrategicCG\n",
    "from chaos.recommend.evaluate.evaluator import LFMEvaluator, PredictionGraphEvaluator, Evaluator\n",
    "from chaos.recommend.predict.predictor import LFMPredictor\n",
    "from chaos.recommend.predict.reciprocal import ReciprocalWrapper\n",
    "from chaos.recommend.translator import LFMTranslator\n",
    "from chaos.shared.model import DataModel\n",
    "from examples.gitchaos.extract import GitHubPreprocessor\n",
    "from examples.gitchaos.fetch import GitHubSource\n",
    "\n",
    "with open('res/logging.yml') as logging_cfg_file:\n",
    "    logging_cfg = yaml.safe_load(logging_cfg_file)\n",
    "    logging.config.dictConfig(logging_cfg)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "The ultimate goal of this scenario is to create your own GitHub universe for reciprocal recommendations.\n",
    "Let's have a look at some GitHub interactions first and how they are weighted approximately within this scenario:\n",
    "\n",
    "![GitHub interactions](img/interactions-gh.png)\n",
    "\n",
    "As we have a symmetric interaction (collaboration), GitHub is a good choice for testing our RRS framework.\n",
    "\n",
    "For this to work, you first need to get a personal access token to be authorized to use the new **GitHub GraphQL API** to use 5000 requests per hour.\n",
    "Follow [these instructions](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token) to get your personal access token **with the scope `user:email`** for fetching user-related info. \n",
    "\n",
    "Insert the token below in `YOUR_TOKEN` with your username as `START_NODE`.\n",
    "If you want a big universe, choose 5000 `NODES`. Beware that this would take a long time.\n",
    "If you don't want to wait, leave the `NODES` as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_TOKEN = \"ghp-...\"\n",
    "START_NODE = 'torvalds' # Change this to your GitHub username\n",
    "NODES = 500\n",
    "\n",
    "BREADTH = 7 # Smaller values will make the algorithm go farther away from your home planet, the start node\n",
    "VISUALIZE_WITH_AVATARS = True # See avatars in your universe\n",
    "FILENAME = f\"gh-{START_NODE}@{NODES}\" # For persisting the file for later usage, no need to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready to start **Reciprocal BFS**? Pushing play will start the algorithm and generate a network dependent on the above parameterization. Then, it will persist the model so that you don't loose it in subsequent runs.\n",
    "\n",
    "This might take some time (about 3-5 minutes with `50 MBit` for `500` nodes, depending on GitHub's response times). The requests are not parallelized, as we do not want to start a DoS or go against GitHub's ToS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Start username: {START_NODE} / Breadth: {BREADTH} / Nodes: {NODES}\")\n",
    "src = GitHubSource(gql_spec=yaml.safe_load(open('./res/gql-spec.yml')),\n",
    "                   token=YOUR_TOKEN,\n",
    "                   start_user=START_NODE,\n",
    "                   breadth=BREADTH, max_nodes=NODES)\n",
    "checkpoint_path = Path(f'res/{FILENAME}')\n",
    "data = None\n",
    "if checkpoint_path.exists():\n",
    "    data = DataModel.load(checkpoint_path)\n",
    "    src.data = data\n",
    "else:\n",
    "    data = src.source_data()\n",
    "    data.save(checkpoint_path)\n",
    "print(\"Done!\")\n",
    "\n",
    "# The following is recommended to remove nodes that are not fully processed from the graph:\n",
    "print(\"Synchronizing user_df with interaction_graph...\")\n",
    "data.sync_graph(inplace=True)\n",
    "# Uncomment only if you are bored, drawing many nodes will take a long time:\n",
    "# data.interaction_graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look at the acquired data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()\n",
    "data.user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Next, we define and run a data pipeline to process user profile features, as illustrated in the following image:\n",
    "\n",
    "![GitHub Feature Engineering Pipeline](img/feat-eng-gh.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SequentialDataPipeline([\n",
    "    SequentialDataPipeline(name='Metadata Preparation Pipeline', processors=[\n",
    "        GitHubPreprocessor(skills_per_user=25, programming_languages_per_user=6),\n",
    "        DFCleaner(['bio'], fill_na_val=''),\n",
    "        DFCleaner(['company', 'location'], str_clean_regex=r'[-.,;+]', fill_na_val=''),\n",
    "        TextConverter('bio', ColumnFormatType.MARKDOWN),\n",
    "        NLPEntityExtractor('bio', {'GPE': 'location_tags', 'LOC': 'location_tags', 'LANGUAGE': 'location_tags',\n",
    "                                   'ORG': 'org_tags', 'PRODUCT': 'org_tags', 'NORP': 'org_tags'}),\n",
    "    ]),\n",
    "    SequentialDataPipeline(name='User Metadata', processors=[\n",
    "        SequentialDataPipeline(name='Bio', processors=[\n",
    "            NLPTokenExtractor('bio', 'bio_tags'),\n",
    "            MostUsedExtractor('bio_tags', 'bio_tags', usage_threshold=2),\n",
    "        ]),\n",
    "        SequentialDataPipeline(name='Organizations', processors=[\n",
    "            NLPTokenExtractor('company', 'org_tags'),\n",
    "            MostUsedExtractor('org_tags', 'org_tags', usage_threshold=2),\n",
    "        ]),\n",
    "        SequentialDataPipeline(name='Location', processors=[\n",
    "            NLPEntityExtractor('company', {'GPE': 'location_tags'}),\n",
    "            NLPTokenExtractor('location', 'location_tags'),\n",
    "            MostUsedExtractor('location_tags', 'location_tags', usage_threshold=2)\n",
    "        ]),\n",
    "        SequentialDataPipeline(name='Process skills', processors=[\n",
    "            NLPEntityExtractor('descriptions', {'%': 'skill_tags'}),\n",
    "            MostUsedExtractor('skills', 'skill_tags', usage_threshold=2),\n",
    "            MostUsedExtractor('programmingLanguages', 'skill_tags', top=40, usage_threshold=2),\n",
    "            MostUsedExtractor('skill_tags', 'skill_tags', top=1000, usage_threshold=2)\n",
    "        ]),\n",
    "    ]),\n",
    "])\n",
    "# We can also simply alter the user_df \"manually\", e.g. to add profile URLs\n",
    "data.user_df['url'] = data.user_df.index.map(lambda u: f'https://github.com/{u}')\n",
    "data = pipeline.execute(data)\n",
    "data.user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Interaction Graph\n",
    "\n",
    "We do not normalize the edges this time and use the strengths as is. If you are curious, you can also adapt this, e.g. compare with the scenario \"Learning Group\".\n",
    "\n",
    "Feel free to experiment!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_pipeline = SequentialDataPipeline(name='Graph Manipulations', processors=[\n",
    "    GraphEdgeMapper(cost=lambda e: 1 / e.strength, capacity=lambda e: e.strength),\n",
    "    GraphPopularityExtractor(target_col='popularity', metrics=('eigenvector', 'degree'),\n",
    "                             labels=['unknown', 'less-known', 'normal', 'well-known', 'popular', 'prominent'],\n",
    "                             quantiles=[0.0, 0.1, 0.4, 0.6, 0.8, 0.99, 1.0], add_as_node_attrib=True)\n",
    "])\n",
    "\n",
    "data = interaction_pipeline.execute(data)\n",
    "data.user_df['popularity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that we have processed that original data, we can test a variation of different `LFMPredictor` configurations.\n",
    "\n",
    "You might need to adapt the `epochs` and/or `hp`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = LFMTranslator(data)\n",
    "# Exlude follower re-recommendations with the following CG:\n",
    "cg = StrategicCG(\n",
    "    InteractionCG(DMCandidateRepo(data), interaction_pattern='follow', include=False),\n",
    "    on_unknown_user=DMCandidateRepo(data)\n",
    ")\n",
    "\n",
    "\n",
    "hp = {'no_components': 48, 'learning_rate': 0.04}\n",
    "# Found by using LFMHyperparameterOptimizer with 250 trials on github-xs, f1 metric, typically beats the above hps with less components!\n",
    "hp_opt = {'no_components': 42, 'learning_rate': 0.0418, 'user_alpha': 1.7007e-05, 'item_alpha': 1.5008e-05}\n",
    "\n",
    "evaluator = LFMEvaluator(\n",
    "    interactions=translator.interaction_matrix,\n",
    "    predictors={\n",
    "        'Hybrid all': LFMPredictor(\n",
    "            LFMTranslator(\n",
    "                data, features={'bio_tags': 0.4, 'location_tags': 0.2, 'skill_tags': 0.2, 'org_tags': 0.2}\n",
    "            ), cg, **hp\n",
    "        ),\n",
    "        'Hybrid all tuned': LFMPredictor(\n",
    "            LFMTranslator(\n",
    "                data, features={'bio_tags': 0.4, 'location_tags': 0.2, 'skill_tags': 0.2, 'org_tags': 0.2}\n",
    "            ), cg, **hp_opt\n",
    "        ),\n",
    "        'Hybrid orgs + bio': LFMPredictor(\n",
    "            LFMTranslator(\n",
    "                data, features=['org_tags', 'bio_tags']\n",
    "            ), cg, **hp\n",
    "        ),\n",
    "        'Collaborative Filtering only': LFMPredictor(translator, **hp_opt)\n",
    "    }\n",
    ")\n",
    "evaluator.run_all(epochs=range(0, 72, 2), metrics=(Evaluator.PRECISION, Evaluator.RECALL, Evaluator.F1))\n",
    "\n",
    "res = evaluator.best_of_all(Evaluator.PRECISION)\n",
    "print(f\"Best predictor for precision: {res.predictor} @ epoch {res.epoch} with {res.value}\")\n",
    "best_predictor = hybrid = evaluator[evaluator.best_of_all('precision').predictor]\n",
    "\n",
    "evaluator.create_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation\n",
    "\n",
    "Let's put the best one inside a `ReciprocalWrapper` to make it return reciprocal recommendations for your own user profile.\n",
    "\n",
    "The following is just a draft. Feel free to change and test anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generating recommendations for you, {START_NODE}...\")\n",
    "\n",
    "rrs = ReciprocalWrapper(best_predictor, ku_factor=8, kv_factor=8)\n",
    "recs = rrs.predict(START_NODE, k=10)\n",
    "\n",
    "print(\"Predictions:\", recs)\n",
    "# print(rrs.stats['rank_violations'])\n",
    "data.user_df.loc[recs.keys()]\n",
    "\n",
    "# Query most similar tags if you want to:\n",
    "# best_predictor.similar_features(['skill_tags:machine-learning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "**Finally, let's create your own GitHub universe made out of latent dimensions!**\n",
    "\n",
    "If launching the *TensorBoard Projector* does not work within your JupyterLab, try to follow the instructions written in the output in a separate terminal.\n",
    "\n",
    "Otherwise, enojoy to explore! (select `PROJECTOR` in the drop-down after TensorBoard has started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VISUALIZE_WITH_AVATARS:\n",
    "    logger.info(\"Download avatar images...\")\n",
    "    src.dl_avatars(base_dir=Path('temp/avatars/'))\n",
    "    logger.info(\"Create sprite with all avatar images...\")\n",
    "    dim = src.create_avatar_sprite(Path('temp/avatars/'), Path(f'temp/avatars/#{FILENAME}.jpeg'))\n",
    "    hybrid.visualize('users', Path(f'temp/avatars/#{FILENAME}.jpeg'), sprite_single_img_dim=dim,\n",
    "                     extra_cols={'url'})\n",
    "else:\n",
    "    hybrid.visualize('users')\n",
    "    \n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
